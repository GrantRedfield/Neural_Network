{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed8f6286-f8c8-426f-bdd0-7ac4002d0afd",
   "metadata": {},
   "source": [
    "## Keras Deep Neural Network\n",
    "\n",
    "Author = Grant Redfield\n",
    "\n",
    "Goal: This script will be used to predict a \"1\" or a \"0\" in the test dataset \"exercise_40_test.csv\" based on 100 different features. I will use a neural network for this prediction\n",
    "\n",
    "Data used: exercise_40_train.csv, exercise_40_test.csv\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4366a1ec-f22e-45a0-8793-aa3738d105fe",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0e15c24-fc05-4506-b2bd-644786a4d947",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import regularizers\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from keras import metrics\n",
    "import gc\n",
    "import os\n",
    "import pickle\n",
    "import keras_tuner   \n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "tf.random.set_seed(42)\n",
    "pd.options.display.max_rows = 400\n",
    "import warnings\n",
    "from keras.models import Model, load_model\n",
    "from keras import backend as K\n",
    "from keras.layers import Input, Dense, Bidirectional\n",
    "from keras.layers import Dropout\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35e62fff-1762-408e-b37c-2df703646d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "SF_Train = pd.read_csv(\"C:\\\\Users\\\\Grant\\\\Desktop\\\\Data_Science\\\\StateFarm\\\\exercise_40_train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0b3426-9c47-47d0-8a4f-6186fae6b08a",
   "metadata": {},
   "source": [
    "### PreProcessing Function + Feature Engineering\n",
    "\n",
    "1. Day of the week had multiple variations of the same day (ex. Sat + Saturday). Normalized this\n",
    "\n",
    "2. Removed Characters like \"$\" and \"%\" to create numeric columns\n",
    "\n",
    "3. Dropped Unnecesary Column \"x39\" which contained the same value throughout the dataframe\n",
    "\n",
    "4. Added New Column considering what part of the week it is (Weekday or Weekend)\n",
    "\n",
    "5. Added New Column considering what time of year it is (Season)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40c096df-264d-457c-9cda-463b91160a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def FixPandas_DF(df):\n",
    "    map_replace = {\n",
    "\"Sat\" : \"Saturday\" ,\n",
    "\"Saturday\" : \"Saturday\" ,\n",
    "\"Sun\" :\"Sunday\" ,\n",
    "\"Mon\" : \"Monday\" ,\n",
    "\"Monday\" : \"Monday\",\n",
    "\"Tue\" : \"Tuesday\",\n",
    "\"Wed\": \"Wednesday\" ,\n",
    "\"Wednesday\": \"Wednesday\" ,\n",
    "\"Thur\" : \"Thursday\",\n",
    "\"Thursday\" : \"Thursday\",\n",
    "\"Fri\": \"Friday\" ,\n",
    "\"Friday\" : \"Friday\"\n",
    "}\n",
    "    df['x3'] = df['x3'].map(map_replace).fillna(SF_Train['x3']) #Point number 1\n",
    "    df['x19'] = df['x19'].str.replace('$', '').astype(float) # Point Number 2\n",
    "    df['x7'] = df['x7'].str.replace('%', '').astype(float) # Point Number 2\n",
    "    df = df.drop(['x39'], axis=1) # Point Number 3\n",
    "   \n",
    "    # Point Number 4\n",
    "    conditions = [\n",
    "    (df['x3'] == 'Saturday') | ( df['x3'] == 'Sunday'),\n",
    "    (df['x3'] == \"Monday\") | (df['x3'] == \"Tuesday\") | (df['x3'] == \"Wednesday\" )| (df['x3'] == \"Thursday\" )| (df['x3'] == \"Friday\" )]\n",
    "    values = ['Weekend', 'Weekday']\n",
    "    df['Part_Of_Week'] = np.select(conditions, values)\n",
    "    conditions = [\n",
    "    (df['x60'] == 'March') | ( df['x60'] == 'April') | ( df['x60'] == 'April'),\n",
    "    (df['x60'] == 'June') | ( df['x60'] == 'July') | ( df['x60'] == 'August'),\n",
    "    (df['x60'] == 'September') | ( df['x60'] == 'October') | ( df['x60'] == 'November'),\n",
    "    (df['x60'] == 'December') | ( df['x60'] == 'January') | ( df['x60'] == 'February')]\n",
    "    values = ['Spring', 'Summer', 'Autumn', 'Winter']\n",
    "    df['Season'] = np.select(conditions, values)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6da030e2-0d95-4bcd-8b90-4ce63b9b9126",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setScaler(x_data, x_values_to_scale):\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(x_data[x_values_to_scale])\n",
    "    return scaler\n",
    "\n",
    "\n",
    "def setOneHotEncoder(x_data, categorical_cols):\n",
    "    \n",
    "    transformer = make_column_transformer((OneHotEncoder( handle_unknown='ignore'), \n",
    "                                       categorical_cols), remainder='passthrough')\n",
    "    one_hot_transformer = transformer.fit(x_data)\n",
    "\n",
    "    with open(\"C:\\\\Users\\\\Grant\\\\Desktop\\\\Data_Science\\\\StateFarm\\\\SF_encoder\", \"wb\") as f: \n",
    "        pickle.dump(one_hot_transformer, f)\n",
    "    \n",
    "    return one_hot_transformer\n",
    "\n",
    "\n",
    "def TransformData(x_data, y_data, numeric_scaler, numeric_cols, categorical_cols, column_means, one_hot_transformer, add_noise):\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    for col in column_means.columns:\n",
    "        x_data[col] = x_data[col].fillna(value = column_means[col][0])\n",
    "    \n",
    "    x_data[numeric_cols] = numeric_scaler.transform(x_data[numeric_cols])\n",
    "    \n",
    "    if add_noise == \"Yes\":\n",
    "        for col in column_means.columns:\n",
    "            x_data[col] = x_data[col].apply(lambda x: x + np.random.normal(0, 0.1,1)[0] )\n",
    "    \n",
    "    x_data[numeric_cols] = x_data[numeric_cols].fillna(0)\n",
    "    \n",
    "    \n",
    "    x_data[categorical_cols] = x_data[categorical_cols].fillna('missing')\n",
    "    x_data[categorical_cols] = x_data[categorical_cols].astype('string')\n",
    "    x_data[categorical_cols] = x_data[categorical_cols].astype('category')\n",
    "    \n",
    "\n",
    "    #print(x_data.isnull().sum())\n",
    "    \n",
    "    x_data_only_cats = one_hot_transformer.transform(x_data)\n",
    "    data_hot_encoded = pd.DataFrame(x_data_only_cats, index=x_data.index)\n",
    "    #Extract only the columns that didnt need to be encoded\n",
    "    x_data = x_data.drop(columns=categorical_cols)\n",
    "    #Concatenate the two dataframes : \n",
    "    x_data = pd.concat([data_hot_encoded, x_data], axis=1)\n",
    "    x_data = x_data.to_numpy()\n",
    "    y_data = y_data.to_numpy()\n",
    "    return x_data, y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4020dcdf-09d4-408e-ba44-493e303fd1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "SF_Train = FixPandas_DF(SF_Train)\n",
    "category_column = ['x3', 'x24','x31','x33','x59','x60','x65','x77','x79','x93','x98','x99','Part_Of_Week','Season']  \n",
    "numeric_cols = SF_Train.columns\n",
    "x_values_to_scale= [x for x in numeric_cols if x not in category_column and x != 'y']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eed0edd-4c46-4528-8eae-f2e0efb65c95",
   "metadata": {},
   "source": [
    "## Up Sample dataset Where Y = 1\n",
    "\n",
    "Since this is a fairly imbalanced dataset, we need to upsample where Y == 1 so our Model can more accurately identify this class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06e50e76-d0ea-4630-84ac-578b2066dd73",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_values = SF_Train[SF_Train['y'] == 1]\n",
    "SF_Train = pd.concat([SF_Train, y_values], ignore_index=True, sort=False)\n",
    "SF_Train = pd.concat([SF_Train, y_values], ignore_index=True, sort=False)\n",
    "SF_Train = pd.concat([SF_Train, y_values], ignore_index=True, sort=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5aa8ab3-dc5b-4f37-b097-47c15db0a340",
   "metadata": {},
   "source": [
    "## Data Transformations\n",
    "\n",
    "1. First I find the means of all training columns to fill in Null values\n",
    "\n",
    "2. create a scalar based on the training data\n",
    "\n",
    "3. Create a OneHotEncoder to change all categorical columns to numeric\n",
    "\n",
    "4. ONLY TRAINING SET: Added Gausssian noise to numeric columns\n",
    "\n",
    "4. Transform All Datasets based on previous steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79a3c7c3-74d3-48f2-86e2-0e614bf51eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_scaler = setScaler(SF_Train.loc[:, SF_Train.columns != 'y'], x_values_to_scale)\n",
    "one_hot_encoder = setOneHotEncoder(SF_Train.loc[:, SF_Train.columns != 'y'], category_column)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    SF_Train.loc[:, SF_Train.columns != 'y'], SF_Train['y'], test_size=0.33, random_state=42)\n",
    "\n",
    "\n",
    "SF_Train_column_means = X_train[x_values_to_scale].mean()\n",
    "SF_Train_column_means = pd.DataFrame(SF_Train_column_means)\n",
    "SF_Train_column_means = SF_Train_column_means.T\n",
    "numeric_scaler = setScaler(X_train.loc[:, X_train.columns != 'y'], x_values_to_scale)\n",
    "one_hot_encoder = setOneHotEncoder(X_train.loc[:, X_train.columns != 'y'], category_column)\n",
    "\n",
    "X_test, X_val, y_test, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.5, random_state=42)\n",
    "\n",
    "X_train, y_train = TransformData(X_train, y_train, numeric_scaler, x_values_to_scale, category_column, SF_Train_column_means, one_hot_encoder, \"Yes\")\n",
    "\n",
    "X_test, y_test = TransformData(X_test, y_test, numeric_scaler, x_values_to_scale, category_column, SF_Train_column_means, one_hot_encoder, \"No\")\n",
    "\n",
    "\n",
    "X_val, y_val = TransformData(X_val, y_val, numeric_scaler, x_values_to_scale, category_column, SF_Train_column_means, one_hot_encoder, \"No\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c6a10d-4d2a-43c9-a9dd-70faf965f602",
   "metadata": {},
   "source": [
    "## Metrics I Will Use To Evaluate My Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b9448d5-5a63-4b7e-9fb4-9c67aa28ec64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a1807b-dfb6-4484-af19-a6f843085b57",
   "metadata": {},
   "source": [
    "## Finding The Best Hyper Parameters Through Bayesian Optimization\n",
    "\n",
    "I will test the activation function and Learning Rate for my Neural Network\n",
    "\n",
    "Previous tests concluded that Dropout Rate should be 0 and activations elu and selu did not perform well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e69d37f-372c-4309-9ee9-3c494f4146cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build(hp):\n",
    "    activation = hp.Choice('activation',\n",
    "                           values=['relu','swish','tanh'],\n",
    "                          ordered=False)\n",
    "           \n",
    "#    D_out = hp.Float(\n",
    "#                       'D_out', \n",
    "#                       min_value=0.0,\n",
    "#                       max_value=0.60,\n",
    "#                       default=0.2)    \n",
    "    learn_r =  hp.Float(\n",
    "        'learn_r',\n",
    "        min_value=1e-7,\n",
    "        max_value=1e-4,\n",
    "        default=1e-6\n",
    "            )    \n",
    "    model = keras.Sequential()\n",
    "    model.add(Dense(300, activation=activation, input_shape=(X_train.shape[1],)))\n",
    "    #model.add(Dropout(D_out))\n",
    "    model.add(Dense(200,  activation=activation))\n",
    "    model.add(Dense(100, activation=activation))\n",
    "    model.add(Dense(50, activation=activation))\n",
    "    model.add(Dense(25, activation=activation))\n",
    "    model.add(Dense(13, activation=activation))\n",
    "    model.add(Dense(5, activation=activation))\n",
    "    model.add(Dense(1,  activation='sigmoid'))   \n",
    "    model.compile(\n",
    "          optimizer=keras.optimizers.Adam(\n",
    "     learning_rate = learn_r\n",
    "\n",
    "        ),\n",
    "            loss='binary_crossentropy',\n",
    "                  metrics=['accuracy',f1_m,precision_m, recall_m, tf.keras.metrics.AUC()],\n",
    "        )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "17e94629-79ee-4b5b-bb3f-eea834a03b1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Search: Running Trial #1\n",
      "\n",
      "Value             |Best Value So Far |Hyperparameter\n",
      "relu              |?                 |activation\n",
      "6.1055e-05        |?                 |learn_r\n",
      "\n",
      "Epoch 1/200\n",
      "1169/1202 [============================>.] - ETA: 0s - loss: 0.6128 - accuracy: 0.6561 - f1_m: 0.4370 - precision_m: 0.5324 - recall_m: 0.4067 - auc: 0.6988"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-f863221cb788>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m bayesian_opt_tuner.search(X_train,y_train,epochs=n_epochs,  callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_accuracy',  \n\u001b[0m\u001b[0;32m     12\u001b[0m                \u001b[0mpatience\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mReduceLROnPlateau\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'val_accuracy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfactor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m                               patience=7, min_lr=0.0001)],\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras_tuner\\engine\\base_tuner.py\u001b[0m in \u001b[0;36msearch\u001b[1;34m(self, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    178\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_trial_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 179\u001b[1;33m             \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_trial\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    180\u001b[0m             \u001b[1;31m# `results` is None indicates user updated oracle in `run_trial()`.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mresults\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras_tuner\\engine\\tuner.py\u001b[0m in \u001b[0;36mrun_trial\u001b[1;34m(self, trial, *args, **kwargs)\u001b[0m\n\u001b[0;32m    292\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_checkpoint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    293\u001b[0m             \u001b[0mcopied_kwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"callbacks\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 294\u001b[1;33m             \u001b[0mobj_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_build_and_fit_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcopied_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    295\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    296\u001b[0m             \u001b[0mhistories\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj_value\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras_tuner\\engine\\tuner.py\u001b[0m in \u001b[0;36m_build_and_fit_model\u001b[1;34m(self, trial, *args, **kwargs)\u001b[0m\n\u001b[0;32m    220\u001b[0m         \u001b[0mhp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrial\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhyperparameters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_try_build\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 222\u001b[1;33m         \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhypermodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    223\u001b[0m         return tuner_utils.convert_to_metrics_dict(\n\u001b[0;32m    224\u001b[0m             \u001b[0mresults\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moracle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobjective\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"HyperModel.fit()\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras_tuner\\engine\\hypermodel.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, hp, model, *args, **kwargs)\u001b[0m\n\u001b[0;32m    135\u001b[0m             \u001b[0mIf\u001b[0m \u001b[1;32mreturn\u001b[0m \u001b[0ma\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mit\u001b[0m \u001b[0mshould\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mthe\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mobjective\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m         \"\"\"\n\u001b[1;32m--> 137\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    138\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1399\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcatch_stop_iteration\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1400\u001b[0m           \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_initial_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_load_initial_step_from_ckpt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1401\u001b[1;33m           \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1402\u001b[0m             with tf.profiler.experimental.Trace(\n\u001b[0;32m   1403\u001b[0m                 \u001b[1;34m'train'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36msteps\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1246\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_insufficient_data\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Set by `catch_stop_iteration`.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1247\u001b[0m         \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1248\u001b[1;33m       \u001b[0moriginal_spe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_steps_per_execution\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1249\u001b[0m       can_run_full_execution = (\n\u001b[0;32m   1250\u001b[0m           \u001b[0moriginal_spe\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    635\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    636\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 637\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    638\u001b[0m     raise NotImplementedError(\n\u001b[0;32m    639\u001b[0m         \"numpy() is only available when eager execution is enabled.\")\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py\u001b[0m in \u001b[0;36mread_value\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    710\u001b[0m     \"\"\"\n\u001b[0;32m    711\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Read\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 712\u001b[1;33m       \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_read_variable_op\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    713\u001b[0m     \u001b[1;31m# Return an identity so it can get placed on whatever device the context\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    714\u001b[0m     \u001b[1;31m# specifies instead of the device where the variable is.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py\u001b[0m in \u001b[0;36m_read_variable_op\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    689\u001b[0m           \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mread_and_set_handle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    690\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 691\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mread_and_set_handle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    692\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    693\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py\u001b[0m in \u001b[0;36mread_and_set_handle\u001b[1;34m()\u001b[0m\n\u001b[0;32m    679\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    680\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread_and_set_handle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 681\u001b[1;33m       result = gen_resource_variable_ops.read_variable_op(\n\u001b[0m\u001b[0;32m    682\u001b[0m           self.handle, self._dtype)\n\u001b[0;32m    683\u001b[0m       \u001b[0m_maybe_set_handle_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\ops\\gen_resource_variable_ops.py\u001b[0m in \u001b[0;36mread_variable_op\u001b[1;34m(resource, dtype, name)\u001b[0m\n\u001b[0;32m    476\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    477\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 478\u001b[1;33m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[0;32m    479\u001b[0m         _ctx, \"ReadVariableOp\", name, resource, \"dtype\", dtype)\n\u001b[0;32m    480\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "bayesian_opt_tuner = keras_tuner.BayesianOptimization(\n",
    "    build,\n",
    "    objective=keras_tuner.Objective(\"val_accuracy\", direction=\"max\"),\n",
    "    max_trials=10,\n",
    "    executions_per_trial=1,\n",
    "    directory=os.path.normpath('C:\\\\Users\\\\Grant\\\\Desktop\\\\Data_Science\\\\StateFarm\\\\'),\n",
    "    project_name='STATEFARM' ,\n",
    "    overwrite=True)\n",
    "n_epochs=200\n",
    "\n",
    "bayesian_opt_tuner.search(X_train,y_train,epochs=n_epochs,  callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_accuracy',  \n",
    "               patience=8), tf.keras.callbacks.ReduceLROnPlateau(monitor='val_accuracy', factor=0.2,\n",
    "                              patience=7, min_lr=0.0001)],\n",
    "     validation_data=(X_val, y_val),verbose=1)\n",
    "\n",
    "\n",
    "bayes_opt_model_best_model = bayesian_opt_tuner.get_best_models(num_models=1)\n",
    "model = bayes_opt_model_best_model[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2985496b-f4a3-4919-8da6-07b3a9fe3f2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "601/601 [==============================] - 5s 8ms/step - loss: 0.0100 - accuracy: 0.9966 - f1_m: 0.9955 - precision_m: 0.9966 - recall_m: 0.9948 - auc: 0.9999\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.009998246096074581,\n",
       " 0.9965682029724121,\n",
       " 0.9954987168312073,\n",
       " 0.9965898394584656,\n",
       " 0.9947759509086609,\n",
       " 0.9998933672904968]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8b1e71-4c89-44fa-87f1-74716250b007",
   "metadata": {},
   "source": [
    "## Neural Network Results\n",
    "\n",
    "The DNN shows to be very accurate showing 0.9999 AUC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cb2e27cb-0286-43b9-8f15-c2e66de09bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TransformFinalData(x_data, numeric_scaler, numeric_cols, categorical_cols, column_means, one_hot_transformer):\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    for col in column_means.columns:\n",
    "        x_data[col] = x_data[col].fillna(value = column_means[col][0])\n",
    "    \n",
    "    x_data[numeric_cols] = numeric_scaler.transform(x_data[numeric_cols])\n",
    "    \n",
    "    x_data[numeric_cols] = x_data[numeric_cols].fillna(0)\n",
    "    \n",
    "    \n",
    "    x_data[categorical_cols] = x_data[categorical_cols].fillna('missing')\n",
    "    x_data[categorical_cols] = x_data[categorical_cols].astype('string')\n",
    "    x_data[categorical_cols] = x_data[categorical_cols].astype('category')\n",
    "    \n",
    "\n",
    "    #print(x_data.isnull().sum())\n",
    "    \n",
    "    x_data_only_cats = one_hot_transformer.transform(x_data)\n",
    "    data_hot_encoded = pd.DataFrame(x_data_only_cats, index=x_data.index)\n",
    "    #Extract only the columns that didnt need to be encoded\n",
    "    x_data = x_data.drop(columns=categorical_cols)\n",
    "    #Concatenate the two dataframes : \n",
    "    x_data = pd.concat([data_hot_encoded, x_data], axis=1)\n",
    "    x_data = x_data.to_numpy()\n",
    "    return x_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a879e04b-2bce-441d-858c-50eb03389f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "SF_TEST = pd.read_csv(\"C:\\\\Users\\\\Grant\\\\Desktop\\\\Data_Science\\\\StateFarm\\\\exercise_40_test.csv\")\n",
    "SF_TEST = FixPandas_DF(SF_TEST)\n",
    "SF_TEST = TransformFinalData(SF_TEST, numeric_scaler, x_values_to_scale, category_column, SF_Train_column_means, one_hot_encoder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7790b7b0-04f5-4b52-9b32-3e2dc2cac914",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "result = model.predict(SF_TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "87bb00ac-fe8e-4fda-b92d-b6df3d6aedc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.DataFrame(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ba3bfdb8-c877-41c6-82a0-97d2a755e3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.to_csv('nonglmresults.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "187ba233-3adf-4923-aba9-73aa906e1d8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
